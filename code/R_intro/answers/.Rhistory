articles_1900_1910_tokenized
articles_1900_1910_tokenized_no_stop_words <- articles_1900_1910_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
# Word Count
count_articles_1900_1910_tokenized_no_stop_words <- articles_1900_1910_tokenized_no_stop_words %>%
count(word, sort=TRUE)
head(count_articles_1900_1910_tokenized_no_stop_words, 20)
articles_1900_1910_bigrams <- articles_1900_1910 %>%
select(sentence) %>%
unnest_tokens(bigram, sentence, token = "ngrams", n = 2)
articles_1900_1910_bigrams_separated <- articles_1900_1910_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
articles_1900_1910_bigrams_separated
count_articles_1900_1910_bigrams <- articles_1900_1910_bigrams_separated %>%
group_by(word1, word2) %>%
count(sort=TRUE)
count_articles_1900_1910_bigrams
articles_1900_1910_bigrams_no_stop <- articles_1900_1910_bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
count_articles_1900_1910_bigrams_no_stop <- articles_1900_1910_bigrams_no_stop %>%
group_by(word1,word2) %>%
count(sort = TRUE) %>%
filter(!is.na(word1))
count_articles_1900_1910_bigrams_no_stop
count_articles_1900_1910_bigrams_no_stop <- count_articles_1900_1910_bigrams_no_stop %>%
mutate(decade = "1900") %>%
select(decade, everything())
count_articles_1900_1910_bigrams_no_stop
###
# Make black press and non-black press dataframes
###
black_press <- lynch %>%
filter(black_press == "Y")
no_black_press <- lynch %>%
filter(is.na(black_press))
###
# Function to make top bigrams for any slice of lynch df
###
make_top_bigrams_df <- function(df, source) {
temp <- df %>%
select(sentence) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!is.na(word1)) %>%
group_by(word1,word2) %>%
count(sort = TRUE, name="count") %>%
filter(!is.na(word1)) %>%
mutate(source = source) %>%
mutate(bigram = paste0(word1, " ", word2)) %>%
ungroup() %>%
select(source,bigram, count) %>%
slice(1:20)
return(temp)
}
###
# Create top bigrams for black press and non-black press
###
black_press_top_bigrams <- make_top_bigrams_df(black_press, "black_press")
non_black_press_top_bigrams <- make_top_bigrams_df(no_black_press, "no_black_press")
###
# Create dataframe of common bigrams in both
###
in_both <- inner_join(black_press_top_bigrams, non_black_press_top_bigrams, by="bigram")
###
# Create dataframe of bigrams exclusive to each
###
in_black_press_only <- anti_join(black_press_top_bigrams, non_black_press_top_bigrams, by="bigram")
in_non_black_press_only <- anti_join(non_black_press_top_bigrams, black_press_top_bigrams, by="bigram")
###
# Write up summary of results
###
summary <- "There are several similarities between the black press and non-black press coverage of lynching, based on the top bigrams for each corpus. Six of 20 bigrams appear in both lists: Mob violence, lynch law, grand jury, white woman, st louis and mob law, suggesting some overlap, though it's hard to know without further research if the terms are used to the same effect in both corpuses.  The black press top list also includes bigrams like colored people, civil rights, anti lynch, the federal government and lynching bill, which are not present in the non-black press top bigrams list. This could suggest that the black press was more likely to discuss the broader social, legal and political implications of lynching, though further research is needed to confirm this. Meanwhile, the list of bigrams exclusive to the non-black press includes several law enforcement and legal process terms -- county jail, deputy sheriff, court house, criminal assault -- and specific references to the act of lynching -- negro lynched, murderer lynched, mob lynches, mob lynched. That could suggest a greater coverage focus of individual lynching events, though further research is needed to confirm this."
###
# Output results
###
black_press_top_bigrams
non_black_press_top_bigrams
in_both
in_black_press_only
in_non_black_press_only
summary
###
# Make black press and non-black press dataframes
###
black_press <- lynch %>%
filter(black_press == "Y")
no_black_press <- lynch %>%
filter(is.na(black_press))
###
# Function to make top bigrams for any slice of lynch df
###
make_top_bigrams_df <- function(df, source) {
temp <- df %>%
select(sentence) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!is.na(word1)) %>%
#group_by(word1,word2) %>%
count(sort = TRUE, name="count") %>%
filter(!is.na(word1)) %>%
mutate(source = source) %>%
mutate(bigram = paste0(word1, " ", word2)) %>%
ungroup() %>%
select(source,bigram, count) %>%
slice(1:20)
return(temp)
}
###
# Create top bigrams for black press and non-black press
###
black_press_top_bigrams <- make_top_bigrams_df(black_press, "black_press")
###
# Make black press and non-black press dataframes
###
black_press <- lynch %>%
filter(black_press == "Y")
no_black_press <- lynch %>%
filter(is.na(black_press))
###
# Function to make top bigrams for any slice of lynch df
###
make_top_bigrams_df <- function(df, source) {
temp <- df %>%
select(sentence) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!is.na(word1)) %>%
count(sort = TRUE, name="count") %>%
filter(!is.na(word1)) %>%
mutate(source = source) %>%
mutate(bigram = paste0(word1, " ", word2)) %>%
ungroup() %>%
select(source,bigram, count) %>%
slice(1:20)
return(temp)
}
###
# Create top bigrams for black press and non-black press
###
black_press_top_bigrams <- make_top_bigrams_df(black_press, "black_press")
View(black_press)
wp <- lynch %>%
filter(is.na(black_press))
stories_tokenized2 <- wp %>%
select(sentence) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 )
View(stories_tokenized2)
stories_tokenized2 <- wp %>%
select(sentence) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_tokenized2
stories <- str_replace_all(wp$sentence, "- ", "")
head(stories)
stories %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories <- str_replace_all(wp$sentence, "- ", "") %>%
as.data.frame()
View(stories)
stories <- str_replace_all(wp$sentence, "- ", "") %>%
as.data.frame() %>%
rename(sentence =1)
x<- stories %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
View(x)
stories_tokenized2 <- wp %>%
select(sentence) %>%
str_replace_all(sentence, "- ", "") %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_tokenized2 <- wp %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_tokenized2
wp_stories_bigrams <- wp %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
wp_stories_bigrams
bp <- lynch %>%
filter(black_press=="Y")
bp_stories_bigrams <- bp %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
bp_stories_bigrams
bp_top5 <- head(bp_stories_bigrams_separated, 5)
bp_top5 <- head(bp_stories_bigrams, 5)
cat("The top five Black press bigrams were:\n")
print(bp_top5)
wp_top5 <- head(wp_stories_bigrams, 5)
cat("The top five white press bigrams were:\n")
print(wp_top5)
# Create a data frame with the top bigrams
results_table <- data.frame(
Rank = 1:5,
`Black Press Bigrams` = bp_top5,
`White Press Bigrams` = wp_top5
)
#fix the column names
results_table <- results_table %>%
rename(bp1 = 2, bp2=3, bp_count=4, wp1=5, wp6=6, wp_count=7)
results_table
library(kableExtra)
# Formatting so you can read it in R studio Dark Mode
kable(results_table,
caption = "Top Five Bigrams in Black and White Press",
align = c('c', 'l', 'l')) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = FALSE,
position = "left",
font_size = 14) %>%
row_spec(0, bold = TRUE, color = "white", background = "#4C4C4C") %>%
column_spec(1, bold = TRUE, border_right = TRUE) %>%
row_spec(1:5, background = "white", color = "black")
newdf <- lynch %>% filter(year >= 1900 & year <= 1910)
newdf
newdf %>%
select(file_id) %>% # I assumed that variable file_id refers to the ID of articles.
# Therefore, counting the number of distinct articles means counting the number of different IDs.
distinct(file_id, .keep_all = TRUE) %>%
count(file_id) %>%
summarize(total =sum(n))
# There are 1322 distinct articles in total.
#rsw comment - yep, that would not be the correct unique id. it would be filename, which yields 1,732 distinct articles in the dataset for the 1900-1910
n_distinct(newdf$filename)
n_distinct(lynch1900s$file_id)
n_distinct(newdf$file_id)
newdf %>%
select(newspaper_state, file_id) %>%
distinct(file_id, .keep_all = TRUE) %>%
count(newspaper_state) %>%
arrange(desc(n))
stories <- str_replace_all(newdf$sentence, "- ", "")
stories_df <- tibble(stories,)
stories_df
# unnest includes lower, punct removal
stories_tokenized <- stories_df %>%
unnest_tokens(word,stories)
stories_tokenized # turned into words
data(stop_words)
test <- stop_words %>%
as.data.frame()
head(test)
stories_tokenized <- stories_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
stories_tokenized
# Word Count
story_word_ct <- stories_tokenized %>%
count(word, sort=TRUE)
head(story_word_ct)
stories_bigrams <- stories_df %>%
unnest_tokens(bigram, stories, token="ngrams", n=2)
stories_bigrams
stories_bigrams_separated <- stories_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
stories_bigrams_separated
dfbi<- stories_bigrams_separated  %>%
count(word1, word2, sort = TRUE)
dfbi
stories_bigrams_filtered <- stories_bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
stories_bigram_cts2 <- stories_bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_bigram_cts2
bdf<- lynch %>%
filter(black_press == "Y")
head(bdf)
bdf
#bdf is the new dataframe including black press articles
nbdf <- lynch %>%
filter(is.na(black_press) | black_press != "Y")
head(nbdf)
# In terms of black press
stories <- str_replace_all(bdf$sentence, "- ", "")
stories_bdf <- tibble(stories,)
stories_bdf # Tokenize
bdf_bigrams <- stories_bdf %>%
unnest_tokens(bigram, stories, token="ngrams", n=2)
bdf_bigrams_separated <- bdf_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")%>%
count(word1, word2, sort = TRUE)
bdf_bigrams_separated
bdf_bigrams_filtered <- bdf_bigrams_separated  %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
bdf_bigrams_filtered
bdf_bigrams_filtered  <- bdf_bigrams_filtered  %>%
filter(!is.na(word1))
bdf_bigrams_filtered
bdf_bigrams_filtered <- bdf_bigrams_filtered %>%
slice_max(n, n=20)
bdf_bigrams_filtered
# In terms of non-black press
stories <- str_replace_all(nbdf$sentence, "- ", "")
stories_nbdf <- tibble(stories,)
stories_nbdf # Tokenize
nbdf_bigrams <- stories_nbdf %>%
unnest_tokens(bigram, stories, token="ngrams", n=2)
nbdf_bigrams_separated <- nbdf_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")%>%
count(word1, word2, sort = TRUE)
nbdf_bigrams_separated
nbdf_bigrams_filtered <- nbdf_bigrams_separated  %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
nbdf_bigrams_filtered
nbdf_bigrams_filtered  <- nbdf_bigrams_filtered  %>%
filter(!is.na(word1))
nbdf_bigrams_filtered
nbdf_bigrams_filtered <- nbdf_bigrams_filtered %>%
slice_max(n, n=20)
nbdf_bigrams_filtered
stories_bigrams_cts3 <- lynch %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_bigrams_cts3
lynch %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
filter(bigram != "temp_file") %>%
filter(bigram != "stories_corpus") %>%
filter(!grepl('[0-9]', bigram))
stories_bigrams_cts3 <- lynch %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
filter(bigram != "temp_file") %>%
filter(bigram != "stories_corpus") %>%
filter(!grepl('[0-9]', bigram))  %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_bigrams_cts3
lynch %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 )
lynch %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
anti_join(stop_words, by = c("word" = "bigram"))
lynch %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
anti_join(stop_words, by = c("bigram" = "word"))
stories_bigrams_cts3 <- lynch %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
anti_join(stop_words, by = c("bigram" = "word")) %>%
filter(bigram != "temp_file") %>%
filter(bigram != "stories_corpus") %>%
filter(!grepl('[0-9]', bigram))  %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_bigrams_cts3
lynch %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories <- str_replace_all(lynch1910$sentence, "- ", "")
lynch1910 <-  lynch %>%
filter(year >= 1900 & year <= 1910)
stories <- str_replace_all(lynch1910$sentence, "- ", "")
stories <- str_replace_all(lynch1910$sentence, "- ", "") %>%
as.tibble()
stories <- str_replace_all(lynch1910$sentence, "- ", "") %>%
as_tibble()
stories_bigrams_cts3 <- lynch %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", ""))
stories_bigrams_cts3 <- lynch1910 %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_bigrams_cts3
stories <- str_replace_all(lynch1910$sentence, "- ", "")
stories_df <- tibble(stories,)
# unnest includes lower, punct removal
stories_tokenized <- stories_df %>%
unnest_tokens(word,stories)
stories_tokenized
stories_tokenized <- stories_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
# Word Count
story_word_ct <- stories_tokenized %>%
count(word, sort=TRUE)
head(story_word_ct)
#write_csv(lynch_word_ct, "lynching_corpus_word_count.csv")
stories_bigrams <- stories_df %>%
unnest_tokens(bigram, stories, token="ngrams", n=2)
stories_bigrams_separated <- stories_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
stories_bigram_cts <- stories_bigrams_separated %>%
count(word1, word2, sort = TRUE)
stories_bigram_cts
stories_bigrams_filtered <- stories_bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
stories_bigram_cts2 <- stories_bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_bigram_cts2
stories_bigrams_cts3 <- lynch1910 %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) %>%
mutate(decade = "1900s")
stories_bigrams_cts3
stories_bigrams_cts3 <- lynch1910 %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(total = word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) %>%
mutate(decade = "1900s")
stories_bigrams_cts3 <- lynch1910 %>%
select(sentence) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1)) %>%
mutate(decade = "1900s")
stories_bigrams_cts3
