---
title: "final_final_project"
---

```{r}

###
# Load libraries +
# Open .Renviron file to add API Keys
###

source("r_scripts/r_environment_setup.R")

# Your .Renviron file should open up here.
# It's a text file you can edit.  
# It's where you'll store your API keys so you don't have to put them in your script.
# Make sure the file is git-ignored, so you don't expose your keys to the public. 
# You'll need to add in API keys for OpenAI, Groq, Amazon Bedrock, Google Gemini, and AWS if you want this script to work.
# DO NOT EDIT THE EXAMPLES BELOW IN THE QMD FILE. 
# THIS IS JUST TO SHOW YOU WHAT IT SHOULD LOOK LIKE IN Renviron. 
# Be sure to save the change to the file when you make edits
#OPENAI_API_KEY = "key_here"
#GROQ_API_KEY = "key_here"
#AWS_ACCESS_KEY_ID = "access_key_here"
#AWS_SECRET_ACCESS_KEY = "secret_key_here"
#AWS_REGION = "region_name"
#GEMINI_API_KEY = "key_here"
#GOOGLE_API_KEY = "key_here"

# If you have added or changed keys in your .Renviron file, you have to restart R to make sure they're loaded.
# If you have done that, you'll need to restart R and then reload libraries.
# You can do that by uncommenting the two lines of code below and then running
# If you have not made changes to .Renviron (you only have to do it once, or when your keys change), then leave these uncommented.
# rstudioapi::restartSession()
# source("r_environment_setup.R")

###
# Load training data
###

source("")

```

```{r}
###
# Load test set
###

# Human-classified story set
# If tuning a model, can be split into test and training
# For our purposes, since we're not tuning a model, we'll use the whole set
# test_set <- read_sheet("https://docs.google.com/spreadsheets/d/1zleETBjVMOwLMYTouK7UHWzQx53H2q2oN_S3mj5tqlk/edit?gid=2053814490#gid=2053814490", sheet="test_slash_training_set") %>%
#   janitor::clean_names() %>%
#   # extract four digit number between first _ and first -  in article_id into a new column called year
#   mutate(year = str_extract(article_id, "(?<=_)(\\d{4})(?=-)")) %>%
#   # extract first number from class and put in a new col called class_id
#   mutate(class_id = str_extract(class, "\\d+")) %>%
#   # extract everything but first number from class and put in a new col called class_definition
#   mutate(class_definition = str_remove(class, "\\d+")) %>%
#   # trim whitespace from class_definition and to lower
#   mutate(class_definition = tolower(str_squish(class_definition))) %>%
#   select(article_id, year, class_id, class_definition)


# use map_dfr to read in feather file for each year stored at build_american_stories_dataset/data_by_year/arrow/articles_YEAR.feather and bind them together
# create a list of distinct years
# years <- test_set %>% distinct(year) %>% pull(year)
# initiate paralell rig
# plan(multisession)
# test_set_articles <- future_map_dfr(years, ~arrow::read_feather(paste0("build_american_stories_dataset/data_by_year/arrow/articles_", .x, ".feather"))) %>%
#  inner_join(test_set, by=c("article_id"="article_id"))


# Write out test_set_articles to a feather file
#arrow::write_feather(test_set_articles, "test_set_articles.feather")
# Read in to save time later
test_set_articles <- arrow::read_feather("test_set_articles.feather")

#test_set_articles <- test_set_articles #%>%
 # slice(1:5)


test_set <- read_sheet("https://docs.google.com/spreadsheets/d/1zleETBjVMOwLMYTouK7UHWzQx53H2q2oN_S3mj5tqlk/edit?gid=2053814490#gid=2053814490", sheet="test_slash_training_set") %>%
  janitor::clean_names() %>%
  mutate(year = str_extract(article_id, "(?<=_)(\\d{4})(?=-)")) %>%
  mutate(class_id = str_extract(class, "\\d+")) %>%
  mutate(class_definition = str_remove(class, "\\d+")) %>%
  mutate(class_definition = tolower(str_squish(class_definition))) %>%
  select(article_id, year, class_id, class_definition)

years <- test_set %>% distinct(year) %>% pull(year)

plan(multisession)

test_set_articles <- future_map_dfr(years, ~arrow::read_feather(paste0("../data/01_raw_articles_by_year/articles_", .x, ".feather"))) %>%
  inner_join(test_set, by=c("article_id"="article_id"))

x <- test_set_articles %>%
  relocate(year, .before=date) %>%
  select(-byline) %>%
  rename(actual_class_id = class_id) 

write_rds(x,"../data/03_test_data/03_test_data.rds")
arrow::write_feather(test_set_articles, "test_set_articles.feather")

test_set_articles <- arrow::read_feather("test_set_articles.feather")
```

```{r}
###
# Build system prompt
###

system_prompt_value <- "You are an expert in classifying historical newspaper articles about lynchings in the United States between 1865 and 1922. You always follow instructions.
I will give you the text of a newspaper article and an associated article_id. The text can classified into one of six distinct categories:
1. An article that describes a specific lynching event that has already happened.
2. An article that does not describe a specific lynching event that has already happened, but does suggest a lynching event may happen in the future. 
3. An article that does not describe a specific lynching event that has already happened, does not suggest a lynching event may happen in the future, but is about federal, state or local policies or laws governing lynching or describes debate over proposed laws.
4. An article that contains strings or partial strings typically found in stories associated with lynching -- like the word 'lynching' or 'lynch' -- but does not describe past or possible lynching events or lynching laws and policies. This could include an article that mentions someone whose last name is Lynch, or a reference toa city that includes 'lynch' as part of its name, like Lynchburg, Va.
5. An article that contains no strings or partial strings typically found in stories associated with lynching and not describe past or possible lynching events or lynching laws and policies.
6. An article that does not fit into any of the first five categories.
Please do the following:
-- The article text provided here was extracted from newspaperpage images through an imperfect OCR process. Do your best to correct any flaws introduced in this process, without changing meaning of the article. You should spellcheck the text and correct spelling errors, standardize capitalization, fix extraneous spaces, remove newline characters and random slashes, separate words that have obviously been concatenated in error, remove non alphabetic or standard punctuation characters. Of special importance is to correct any errors that will prevent the json from being parsed correctly later. 
-- Select the category that best describes the article text. Choose only one. 
-- Develop a brief explanation of why you chose a specific category, including keywords or terms that support the decision.

Format your response as a JSON object with these exact fields:
{
    \"article_id\": \"string, unchanged from input\",
    \"spellchecked_text\": \"string, corrected spelling of article\",
    \"category_id\": \"string, single digit 1-6\",
    \"category_description\": \"string, exact category description from above\",
    \"explanation\": \"string, brief reason for classification\"
}

Important formatting rules:
- Use double quotes for all strings
- No line breaks in text fields
- No trailing commas
- No comments or additional text
- No markdown formatting
- Escape all quotes within text using single quotes
- Remove any \\r or \\n characters from text
- End the JSON object with a single closing curly brace }"

```

```{r}
count_responses <- function() {
  # Get all folders in llm_responses
  providers <- list.dirs("llm_responses", recursive = FALSE, full.names = FALSE)
  
  results <- data.frame()
  
  for (provider in providers) {
    # Get model folders for this provider
    model_folders <- list.dirs(file.path("llm_responses", provider), 
                             recursive = FALSE, 
                             full.names = FALSE)
    
    for (model in model_folders) {
      # Full path to check
      full_path <- file.path("llm_responses", provider, model)
      
      # Count .rds files (responses) and exclude error_log.csv
      file_count <- length(list.files(full_path, pattern = "\\.rds$"))
      
      # Add to results
      results <- rbind(results, data.frame(
        provider = provider,
        model = model,
        responses = file_count
      ))
    }
  }
  
  return(results)
}

unfinished_file_counts <- count_responses() %>%
  filter(responses != max(responses))

unfinished_file_count_models <- unfinished_file_counts %>%
  mutate(model = paste0(provider, "_", model)) %>%
  #slice(3,4) %>%
  pull(model)

```

```{r}
###
# Define test function
###
library(furrr)
#sample_size <-1
#model_provider_type <- "openai_gpt-4o"
#workers <- 1
#i <- 1
classify_articles <- function(model_provider_type, sample_size=1, overwrite=FALSE) {
  if (!is.character(model_provider_type) || length(model_provider_type) != 1) {
    stop("model_provider_type must be a single character string")
  }
  
  parts <- str_split(model_provider_type, "_")[[1]]
  if (length(parts) != 2) stop("model_provider_type must be in format 'provider_model'")
  
  model_provider <- parts[1]
  model_type <- parts[2]
  
  save_dir <- file.path("llm_responses", model_provider, model_type)
  dir.create(save_dir, recursive = TRUE, showWarnings = FALSE)
  
  log_file <- file.path(save_dir, "error_log.csv")
  #if (overwrite || !file.exists(log_file)) {
    write.csv(data.frame(
      timestamp = character(),
      article_id = character(),
      error = character(),
      stringsAsFactors = FALSE
    ), log_file, row.names = FALSE)
  #}
  
  result_df <- test_set_articles %>% slice(1:sample_size)
  articles <- result_df %>% pull(article)
  article_ids <- result_df %>% pull(article_id)
  #i <- 1
  process_article <- function(i) {
    file_path <- file.path(save_dir, paste0(article_ids[i], ".rds"))
    if (!overwrite && file.exists(file_path)) {
      print(paste("Article", article_ids[i], "already processed"))
      return(NULL)
    }
    
    print(paste("Article", article_ids[i], "not yet processed, starting..."))

    chat <- if (model_provider == "ollama") {
      chat_ollama(model = model_type, system_prompt = system_prompt_value, echo = FALSE)
    } else if (model_provider == "groq") {
      chat_groq(model = model_type, system_prompt = system_prompt_value, echo = FALSE)
    } else if (model_provider_type %in% c("openai_o1-preview","openai_o1-mini")) {
      chat_openai(model = model_type, echo = FALSE)
    } else if (model_provider == "openai") {
      chat_openai(model = model_type, system_prompt = system_prompt_value, echo = FALSE)
    } else if (model_provider == "gemini") {
      chat_gemini(model = model_type, system_prompt = system_prompt_value, echo = FALSE)
    } else if (model_provider == "bedrock") {
      chat_bedrock(model = model_type, echo = FALSE)
    }
    
    text <- paste0("Article ID: ", article_ids[i], "\n\nArticle: ", articles[i])
    if (model_provider == "bedrock" | model_provider_type %in% c("openai_o1-preview","openai_o1-mini")) {
      text <- paste0("Instructions: ", system_prompt_value, "\nArticle ID: ", article_ids[i], "\nArticle to process: ", articles[i])
    }
    
    tryCatch({
      response <- chat$chat(text)
      saveRDS(response, file = file_path)
    }, error = function(e) {
      error_msg <- as.character(e)
      write.table(
        data.frame(
          timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
          article_id = article_ids[i],
          error = error_msg,
          stringsAsFactors = FALSE
        ),
        log_file,
        sep = ",",
        row.names = FALSE,
        col.names = !file.exists(log_file),
        append = TRUE
      )
      warning(sprintf("Row %d: Failed to get response for %s model %s.\nError: %s",
                     i, model_provider, model_type, error_msg))
    })
  }
  
  lapply(seq_along(articles), process_article)
  print(paste0("finished processing ", model_provider_type))
}

overwrite <- FALSE
model_provider_type <- "gemini_gemini-1.5-pro"
sample_size <- nrow(test_set_articles)
classify_articles(model_provider_type, sample_size, overwrite)

model_provider_type_list <- c(
  # Pass 1 
  "openai_gpt-4o",
  "groq_gemma2-9b-it",
  "bedrock_anthropic.claude-3-5-sonnet-20240620-v1:0",
  
  # Pass 2
  "openai_gpt-4o-mini", # works
  "groq_gemma-7b-it",# works
  "bedrock_anthropic.claude-v2:1", # works
  

  # Pass 3  
  "openai_gpt-4-turbo", # works
  "groq_llama3-groq-70b-8192-tool-use-preview",
  "bedrock_anthropic.claude-v2",# works
  

  # Pass 4
  "openai_gpt-4-turbo-preview", # works
  "groq_llama3-groq-8b-8192-tool-use-preview",
  "bedrock_anthropic.claude-instant-v1", # works
  
  # Pass 5
  "openai_gpt-3.5-turbo", # works
  "groq_llama-3.1-70b-versatile",# works
  "bedrock_anthropic.claude-3-haiku-20240307-v1:0", # works

  # Pass 6
  "openai_o1-preview",# works  
  "groq_llama-3.1-8b-instant",# works
  "bedrock_anthropic.claude-3-sonnet-20240229-v1:0", # works

  
  # Pass 7
  "openai_o1-mini",# works
  "groq_llama-3.2-1b-preview",# works

  # Pass 8
  "groq_llama-3.2-3b-preview",# works
  "bedrock_cohere.command-text-v14", # works
  


  # Pass 9 
  "groq_llama3-70b-8192",# works
  "bedrock_cohere.command-light-text-v14", # works
 

  # Pass 10 
  "groq_llama3-8b-8192",# works
  "bedrock_cohere.command-r-v1:0", # works
  

  # Pass 12  
  "groq_mixtral-8x7b-32768",# works
  "bedrock_cohere.command-r-plus-v1:0", # works
  #"gemini_gemini-exp-1114",#works

  # Pass 13
  "bedrock_ai21.jamba-1-5-large-v1:0", # this works
  #"gemini_gemini-exp-1121", #works

  # Pass 14
  "bedrock_ai21.jamba-1-5-mini-v1:0", # this works
  #"gemini_gemini-1.5-pro",
  
  # Pass 15
  "bedrock_meta.llama3-70b-instruct-v1:0", # this works
  #"gemini_gemini-1.5-flash",#works
  
  # Pass 16
  "bedrock_amazon.titan-text-premier-v1:0"#,# this works
  #"gemini_learnlm-1.5-pro-experimental"#works

)

# set workers to 1 less than number of cores
# model_provider_type_list <- c("bedrock_amazon.titan-text-premier-v1:0")

workers <- parallel::detectCores() - 1
future::plan(future::multisession, workers = workers)

model_provider_type_list <- unfinished_file_count_models
# Run classification in parallel
future_map(
  model_provider_type_list,
  classify_articles,
  sample_size = nrow(test_set_articles),
  overwrite = FALSE,
  .progress = TRUE
)
nrow(test_set_articles)
# Reset to sequential processing
#plan(sequential)

```

```{r}

combine_responses <- function(base_dir = "llm_responses") {
  files <- list.files(base_dir, pattern = "\\.rds$", recursive = TRUE, full.names = TRUE)
  
  responses <- map_dfr(files, function(file) {
    parts <- str_split(file, "/")[[1]]
    model_provider <- parts[2]
    model_type <- parts[3]
    article_id <- str_remove(basename(file), "\\.rds$")
    
    raw_response <- readRDS(file)
    response_parts <- str_split(raw_response, "Warning:")[[1]]
    response <- response_parts[1]
    
    json <- str_extract(response, "\\{[^}]*\\}")
    if (is.na(json)) {
      json <- str_extract(response, "\\{.*") %>%
      str_replace_all("\\\\_", "_") %>%
      str_replace_all("\\\\([nrt])", "") %>%
      str_replace_all("\\\\([^\"'\\\\])", "\\1") %>%
      str_replace_all("\\s+", " ") %>%
      str_trim()
      
      json <- paste0(json, "}")
      
    } else {
    #if (is.na(json)) return(create_empty_row(article_id, model_provider, model_type, raw_response))
    
    json <- json %>%
      str_replace_all("\\\\_", "_") %>%
      str_replace_all("\\\\([nrt])", "") %>%
      str_replace_all("\\\\([^\"'\\\\])", "\\1") %>%
      str_replace_all("\\s+", " ") %>%
      str_trim()
      
    }
    
    tryCatch({
      parsed <- jsonlite::fromJSON(json)
      create_row(article_id, model_provider, model_type, json, parsed)
    }, error = function(e) {
      warning(sprintf("Failed to parse JSON for file %s: %s\nResponse: %s", file, as.character(e), json))
      create_empty_row(article_id, model_provider, model_type, json)
    })
  })
  responses
}

# Helper functions
create_empty_row <- function(article_id, model_provider, model_type, raw_response) {
  tibble(
    article_id = article_id,
    model_provider = model_provider,
    model_type = model_type,
    raw_response = raw_response,
    spellchecked_text = NA_character_,
    category_id = NA_character_,
    category_description = NA_character_,
    explanation = NA_character_
  )
}

create_row <- function(article_id, model_provider, model_type, raw_response, parsed) {
  tibble(
    article_id = article_id,
    model_provider = model_provider,
    model_type = model_type,
    raw_response = raw_response,
    spellchecked_text = parsed$spellchecked_text %||% NA_character_,
    category_id = as.character(parsed$category_id) %||% NA_character_,
    category_description = parsed$category_description %||% NA_character_,
    explanation = as.character(parsed$explanation) %||% NA_character_
  )
}
responses <- combine_responses()

z <- responses %>% filter(is.na(spellchecked_text)) %>% pull(raw_response)

z

#readRDS("llm_responses/groq/gemma2-9b-it/3_1889-06-05_p13_sn96027724_00271761466_1889060501_0389.rds")

```

```{r}
quantification <- responses %>%
  mutate(category_id_clean = str_sub(category_id, 1, 1)) %>%
  #filter(category_id_clean %in% c("1","2","3","4","5","6")) %>%
  rename(predicted_class_id = category_id_clean) %>%
  inner_join(test_set_articles, by = "article_id") %>%
  mutate(actual_class_id = class_id) %>%
  select(1:3, predicted_class_id, actual_class_id,class_definition, everything()) %>%
  mutate(accurate_prediction = if_else(predicted_class_id == actual_class_id, "TRUE", "FALSE")) %>%
  mutate(temp=1) %>%
  group_by(model_provider,model_type, accurate_prediction) %>%
  summarise(total=sum(temp)) %>%
  ungroup() %>%
  pivot_wider(names_from=accurate_prediction, values_from=total) %>%
  mutate(total_predictions = `TRUE` + `FALSE`) %>%
  # percent total accurate predictions
  mutate(percent_accurate = (`TRUE`/total_predictions)*100)

```

```{r}


# Human-classified story set
# If tuning a model, can be split into test and training
# For our purposes, since we're not tuning a model, we'll use the whole set
# test_set <- read_sheet("https://docs.google.com/spreadsheets/d/1zleETBjVMOwLMYTouK7UHWzQx53H2q2oN_S3mj5tqlk/edit?gid=2053814490#gid=2053814490", sheet="test_slash_training_set") %>%
#   janitor::clean_names() %>%
#   # extract four digit number between first _ and first -  in article_id into a new column called year
#   mutate(year = str_extract(article_id, "(?<=_)(\\d{4})(?=-)")) %>%
#   # extract first number from class and put in a new col called class_id
#   mutate(class_id = str_extract(class, "\\d+")) %>%
#   # extract everything but first number from class and put in a new col called class_definition
#   mutate(class_definition = str_remove(class, "\\d+")) %>%
#   # trim whitespace from class_definition and to lower
#   mutate(class_definition = tolower(str_squish(class_definition))) %>%
#   select(article_id, year, class_id, class_definition)


# use map_dfr to read in feather file for each year stored at build_american_stories_dataset/data_by_year/arrow/articles_YEAR.feather and bind them together
# create a list of distinct years
# years <- test_set %>% distinct(year) %>% pull(year)

years <- c("1891", "1915", "1913", "1890", "1902")

# Pattern 1: Lynching of colored people
pattern1 <- "lynching[s]?\\W+of\\W+(the\\W+)?(\\w+\\W+){1,2}?colored"
pattern2 <- "(murderer|fiend|desperado|brute)\\W+(\\w+\\W+){1,2}?lynch(ed|es|ing)?(\\W+|$)"
pattern3 <- "colored[s]?\\W+(\\w+\\W+){1,2}?((was|were)\\W+)?lynch(ed|es|ing)?(\\W+|$)"
pattern4 <- "lynching[s]?\\W+of\\W+(the\\W+)?(\\w+\\W+){1,2}?negro"
pattern5 <- "mob\\W+(\\w+\\W+){1,2}?(hung|hang(ed|ings?|s)|lynch(ed|es|ing)?)"
pattern6 <- "negro(e?s)?\\W+(\\w+\\W+){1,2}?((was|were)\\W+)?lynch(ed|es|ing)?(\\W+|$)"
regex_pattern <- paste0(pattern1,"|",pattern2,"|",pattern3,"|",pattern4,"|",pattern5,"|",pattern6)


read_and_filter <- function(year) {
  
  temp <- arrow::read_feather(paste0("build_american_stories_dataset/data_by_year/arrow/articles_", year, ".feather")) %>%
  # make article lowercase, remove all punctuation and numbers
  mutate(article_clean = str_to_lower(article)) %>%
  # remove punct
  mutate(article_clean = str_replace_all(article_clean, "[[:punct:]]", " ")) %>%
  # remove numbers
  mutate(article_clean = str_replace_all(article_clean, "[[:digit:]]", " ")) %>%
  # remove internal whitespace
  mutate(article_clean = str_squish(article_clean)) %>%
  mutate(jack_regex_match = str_detect(article_clean, regex_pattern)) %>%
  filter(jack_regex_match == TRUE) 

  return(temp)  
  
}

plan(multisession, workers = 5)

test_set_articles <- future_map_dfr(years, read_and_filter)

x <- test_set_articles %>%
  mutate(check = if_else(str_detect(article_clean,"orderly|judge lynch|citizen"), "TRUE", "FALSE")) 

sheet_write(x, "https://docs.google.com/spreadsheets/d/1zleETBjVMOwLMYTouK7UHWzQx53H2q2oN_S3mj5tqlk/edit?gid=2053814490#gid=2053814490", sheet="orderly_justice_judge_citizens")




not_lynch <- df_clean %>%
  filter(!str_detect(article_clean,"lynch")) %>%
  filter(jack_regex_match != TRUE)

judge <- df_clean %>%
  filter(str_detect(article_clean,"judge lynch")) 



```
